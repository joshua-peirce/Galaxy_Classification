{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b92250-f3b1-47a2-a7df-2a976efa4bde",
   "metadata": {},
   "source": [
    "# The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb627514-d77c-4487-9c3e-bd00e2c01e56",
   "metadata": {},
   "source": [
    "## Setting up the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a967872a-c466-446d-b0a6-1c31d0039868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5714dfcab0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import trange\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 1234\n",
    "# cuDNN uses nondeterministic algorithms, set some options for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc8656e-8148-4a3d-a4fc-65aca60500b8",
   "metadata": {},
   "source": [
    "This next cell was modified from the pytorch official docs\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbbd5f8-9498-4a9b-b36c-8b2f5b73c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class H5ImageDataset(Dataset):\n",
    "    def __init__(self, data_file, train=True, transform=None, target_transform=None):\n",
    "        self.file = h5py.File(data_file)\n",
    "        self.labels = self.file.get(\"ans\")\n",
    "        self.imgs = self.file.get(\"images\")\n",
    "        self.train = train\n",
    "        if train:\n",
    "            self.index_transform = self.train_index\n",
    "        else:\n",
    "            self.index_transform = self.test_index\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def train_index(self, idx):\n",
    "        \"\"\"Here I had to use some clever math to go from a training set index to an overall set index. It skips every 5th number, starting w/ 0\"\"\"\n",
    "        return (idx // 4) * 5 + (idx % 4) + 1\n",
    "    \n",
    "    def test_index(self, idx):\n",
    "        \"\"\"This is how I got the test indices. Every 5th index in the overall dataset is a testing datapoint, so we get the 5th one.\"\"\"\n",
    "        return idx * 5\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train: \n",
    "            return len(self.labels) // 5 * 4\n",
    "        return len(self.labels) // 5\n",
    "                   \n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.index_transform(idx)\n",
    "        image = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "    def get_mean(self):\n",
    "        return np.mean(self.imgs)\n",
    "    \n",
    "    def get_std(self):\n",
    "        return np.std(self.imgs)\n",
    "    \n",
    "class NPYImageDataset(Dataset):\n",
    "    def __init__(self, imgs_file, labels_file, train=True, transform=None, target_transform=None):\n",
    "        self.labels = np.load(labels_file)\n",
    "        self.imgs = np.load(imgs_file)\n",
    "        self.train = train\n",
    "        if train:\n",
    "            self.index_transform = self.train_index\n",
    "        else:\n",
    "            self.index_transform = self.test_index\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def train_index(self, idx):\n",
    "        \"\"\"Here I had to use some clever math to go from a training set index to an overall set index. It skips every 5th number, starting w/ 0\"\"\"\n",
    "        return (idx // 4) * 5 + (idx % 4) + 1\n",
    "    \n",
    "    def test_index(self, idx):\n",
    "        \"\"\"This is how I got the test indices. Every 5th index in the overall dataset is a testing datapoint, so we get the 5th one.\"\"\"\n",
    "        return idx * 5\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train: \n",
    "            return len(self.labels) // 5 * 4\n",
    "        return len(self.labels) // 5\n",
    "                   \n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.index_transform(idx)\n",
    "        image = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "    def get_mean(self):\n",
    "        return np.mean(self.imgs)\n",
    "    \n",
    "    def get_std(self):\n",
    "        return np.std(self.imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3315e72e-23cf-4b51-851f-1c9b37d87820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5714dfcab0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = NPYImageDataset(\"sample_imgs_large.npy\", \"sample_labels_large.npy\", train=True)\n",
    "test_data  = NPYImageDataset(\"sample_imgs_large.npy\", \"sample_labels_large.npy\", train=False)\n",
    "print(len(train_data))\n",
    "batch_size = 1\n",
    "seed = 123\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45e3325-dc8e-482f-aa4d-2be2eb697c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0., 0., 0.), std=(0.2, 0.2, 0.2))\n",
    "])\n",
    "\n",
    "train_data.transform = transform\n",
    "test_data.transform = transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f87da6-8e24-4445-8aca-f27110bdd74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dataloader objects\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,  num_workers=True) \n",
    "test_loader  = torch.utils.data.DataLoader(test_data,  batch_size=batch_size, shuffle=False, num_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61ff5e-8ed7-4830-ae4d-357c3d90bbf7",
   "metadata": {},
   "source": [
    "## Setting up the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94796f66-5928-475f-aeff-f14030b77e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(20, 50, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (maxpool3): MaxPool2d(kernel_size=(5, 5), stride=(5, 5), padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=1250, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (logSoftmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Conv2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Flatten\n",
    "from torch.nn import Linear\n",
    "from torch.nn import LogSoftmax\n",
    "\n",
    "class CNNClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=10,\n",
    "            kernel_size=(3, 3))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        \n",
    "        self.conv2 = Conv2d(in_channels=10, out_channels=20,\n",
    "            kernel_size=(2, 2))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        \n",
    "        self.conv3 = Conv2d(in_channels=20, out_channels=50,\n",
    "            kernel_size=(3, 3))\n",
    "        self.relu3 = ReLU()\n",
    "        self.maxpool3 = MaxPool2d(kernel_size=(5, 5), stride=(5, 5))\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Linear(in_features=1250, out_features=50)\n",
    "        self.relu3 = ReLU()\n",
    "        \n",
    "        self.fc2 = Linear(in_features=50, out_features=10)\n",
    "        self.logSoftmax = LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ------------------\n",
    "        # Write your implementation here.        \n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)  \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logSoftmax(x)\n",
    "        \n",
    "        y_output = x\n",
    "        \n",
    "        return y_output\n",
    "        # ------------------\n",
    "\n",
    "model = CNNClassifier().to(DEVICE)\n",
    "\n",
    "# sanity check\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b86322-63c5-4f0d-94c0-860c86cbbc86",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3b104c1-a338-4c7c-9811-05bb23f7ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train one epoch\n",
    "def train_one_epoch(train_loader, model, device, optimizer, log_interval, epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    counter = []\n",
    "    \n",
    "    for i, (img, label) in enumerate(train_loader):\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img)\n",
    "        #print(output, output.shape)\n",
    "        loss = torch.nn.functional.nll_loss(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Record training loss every log_interval and keep counter of total training images seen\n",
    "        if (i+1) % log_interval == 0:\n",
    "            losses.append(loss.item())\n",
    "            counter.append(\n",
    "                (i * batch_size) + img.size(0) + epoch * len(train_loader.dataset))\n",
    "\n",
    "    return losses, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c86c16-79b6-4e09-a89c-4afcd099727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to test one epoch\n",
    "def test_one_epoch(test_loader, model, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    num_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (img, label) in enumerate(test_loader):\n",
    "            img, label = img.to(device), label.to(device)\n",
    "\n",
    "            # ------------------\n",
    "            # Write your implementation here.\n",
    "            \n",
    "            output = model(img)\n",
    "            pred = output.max(1)[1] # Get index of largest log-probability and use that as prediction\n",
    "            num_correct += pred.eq(label).sum().item()\n",
    "            test_loss /= len(test_loader)\n",
    "            # ------------------\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss, num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ae286c1-2663-4f43-9969-3ddfde251d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|███████████████████████████████████| 10/10 [00:32<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.1485148514851485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "max_epochs=10\n",
    "\n",
    "# Recording data\n",
    "log_interval = 100\n",
    "\n",
    "# Instantiate optimizer (model was created in previous cell)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_correct = []\n",
    "for epoch in trange(max_epochs, leave=True, desc='Epochs'):\n",
    "    train_loss, counter = train_one_epoch(train_loader, model, DEVICE, optimizer, log_interval, epoch)\n",
    "    test_loss, num_correct = test_one_epoch(test_loader, model, DEVICE)\n",
    "\n",
    "    # Record results\n",
    "    train_losses.extend(train_loss)\n",
    "    train_counter.extend(counter)\n",
    "    test_losses.append(test_loss)\n",
    "    test_correct.append(num_correct)\n",
    "\n",
    "print(f\"Test accuracy: {test_correct[-1]/len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0325c300-2bd4-402d-b778-6c4cf274e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_output.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
